<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="November 15, 2016" />
  <title>Workshop on Parallelization and Campus Computing Resources (Part 2)</title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" type="text/css" media="screen, projection, print"
    href="http://www.w3.org/Talks/Tools/Slidy2/styles/slidy.css" />
  <script src="http://www.w3.org/Talks/Tools/Slidy2/scripts/slidy.js"
    charset="utf-8" type="text/javascript"></script>
</head>
<body>
<div class="slide titlepage">
  <h1 class="title">Workshop on Parallelization and Campus Computing Resources (Part 2)</h1>
  <p class="author">
November 15, 2016
  </p>
  <p class="date">Chris Paciorek, Department of Statistics and Berkeley Research Computing, UC Berkeley</p>
</div>
<div id="this-workshop" class="slide section level1">
<h1>This workshop</h1>
<p>Session 1 on November 8 gave an overview of computing and data storage resources available through UC Berkeley and information on using the SCF and Savio Linux clusters.</p>
<p>Session 2 on November 15 covers strategies for parallelizing your work, key concepts in implementing parallelization, and details of using parallelization in Python, R, MATLAB and C++.</p>
<p>Savio is the campus Linux high-performance computing cluster, run by <a href="http://research-it.berkeley.edu/programs/berkeley-research-computing">Berkeley Research Computing</a>.</p>
<p>This tutorial assumes you have a working knowledge of bash and a scripting language such as Python, R, MATLAB, or Julia.</p>
<p>Materials for this tutorial, including the Markdown file and associated code files that were used to create this document are available on Github at https://github.com/berkeley-scf/parallel-scf-2016. You can download the files by doing a git clone from a terminal window on a UNIX-like machine, as follows:</p>
<pre><code>git clone https://github.com/berkeley-scf/parallel-scf-2016</code></pre>
<p>The materials are also available as a <a href="https://github.com/berkeley-scf/parallel-scf-2016/archive/master.zip">zip file</a>.</p>
<p>This material by Christopher Paciorek is licensed under a Creative Commons Attribution 3.0 Unported License.</p>
</div>
<div id="learning-resources-and-links" class="slide section level1">
<h1>Learning resources and links</h1>
<p>This workshop is based largely on already-prepared SCF and BRC (Berkeley Research Computing) material and other documentation that you can look at for more details:</p>
<p>Information on parallel coding (for Session 2)</p>
<p>Session 2 is essentially a subset/rearrangement of material in these SCF tutorials:</p>
<ul>
<li><a href="https://github.com/berkeley-scf/tutorial-parallel-basics">Tutorial on shared memory parallel processing</a>, in particular the <a href="https://rawgit.com/berkeley-scf/tutorial-parallel-basics/master/parallel-basics.html">HTML overview</a></li>
<li><a href="https://github.com/berkeley-scf/tutorial-parallel-distributed">Tutorial on distributed memory parallel processing</a>, in particular the <a href="https://rawgit.com/berkeley-scf/tutorial-parallel-distributed/master/parallel-dist.html">HTML overview</a></li>
</ul>
</div>
<div id="outline" class="slide section level1">
<h1>Outline</h1>
<ul>
<li>Overview of parallel processing approaches</li>
<li>Strategies for effective parallelization</li>
<li>Basic parallelized code on one machine
<ul>
<li>general considerations</li>
<li>foreach, parLapply, mclapply in R</li>
<li>iPython parallel</li>
</ul></li>
<li>Basic parallelizaton across multiple machines
<ul>
<li>foreach, parLapply in R</li>
<li>iPython parallel</li>
</ul></li>
<li>Random number generation in parallel jobs</li>
<li>MPI
<ul>
<li>Overview</li>
<li>Basic usage in C/C++</li>
<li>MPI via mpi4py and Rmpi</li>
</ul></li>
</ul>
</div>
<div id="taxonomy-of-parallel-processing" class="slide section level1">
<h1>Taxonomy of parallel processing</h1>
<p>There are two basic flavors of parallel processing (leaving aside GPUs): distributed memory and shared memory. With shared memory, multiple processors (which I'll call cores) share the same memory. With distributed memory, you have multiple nodes, each with their own memory. You can think of each node as a separate computer connected by a fast network.</p>
<h2 id="some-useful-terminology">Some useful terminology:</h2>
<ul>
<li><em>cores</em>: We'll use this term to mean the different processing units available on a single node.</li>
<li><em>nodes</em>: We'll use this term to mean the different computers, each with their own distinct memory, that make up a cluster or supercomputer.</li>
<li><em>processes</em>: computational tasks executing on a machine; multiple processes may be executing at once. A given program may start up multiple processes at once. Ideally we have no more processes than cores on a node.</li>
<li><em>threads</em>: multiple paths of execution within a single process; the OS sees the threads as a single process, but one can think of them as 'lightweight' processes. Ideally when considering the processes and their threads, we would have no more processes and threads combined than cores on a node.</li>
<li><em>forking</em>: child processes are spawned that are identical to the parent, but with different process IDs and their own memory.</li>
<li><em>sockets</em>: some of R's parallel functionality involves creating new R processes (e.g., starting processes via <em>Rscript</em>) and communicating with them via a communication technology called sockets.</li>
</ul>
</div>
<div id="shared-memory" class="slide section level1">
<h1>Shared memory</h1>
<p>For shared memory parallelism, each core is accessing the same memory so there is no need to pass information (in the form of messages) between different machines. But in some programming contexts one needs to be careful that activity on different cores doesn't mistakenly overwrite places in memory that are used by other cores.</p>
<p>Two standard forms of parallelization on a single machine are:</p>
<ul>
<li>threaded code
<ul>
<li>openMP is the standard protocol for having pieces of a C/C++ program operate on multiple cores</li>
<li>modern, fast BLAS linear algebra packages such as openBLAS, MKL and ACML are threaded</li>
</ul></li>
<li>multicore functionality
<ul>
<li>this involves starting up new processes (computational engines) and dispatching computational tasks to them</li>
</ul></li>
</ul>
<h2 id="threading">Threading</h2>
<p>Threads are multiple paths of execution within a single process. If you are monitoring CPU usage (such as with <em>top</em> from the Linux/Mac command line) and watching a job that is executing threaded code, you'll see the process using more than 100% of CPU. When this occurs, the process is using multiple cores, although it appears as a single process rather than as multiple processes.</p>
</div>
<div id="distributed-memory" class="slide section level1">
<h1>Distributed memory</h1>
<p>Parallel programming for distributed memory parallelism requires passing messages between the different nodes. The standard protocol for doing this is MPI, of which there are various versions, including <em>openMPI</em>, which we'll use here.</p>
<p>You can write your own C/C++ code using MPI or use MPI via R or Python. The R package <em>Rmpi</em> implements MPI in R. The <em>pbdR</em> packages for R also implement MPI as well as distributed linear algebra. Python has a package <em>mpi4py</em> that allows use of MPI within Python.</p>
<p>More simply, in both R and Python, there are easy ways to do embarrassingly parallel calculations (such as simple parallel for loops) across multiple machines, with MPI and similar tools used behind the scenes to manage the worker processes.</p>
<p>In summary, some types of distributed memory parallelization are:</p>
<ul>
<li>simple parallelization of embarrassingly parallel computations (in R, Python, and Matlab) without writing code that explicitly uses MPI;</li>
<li>distributed linear algebra using the <em>pbdR</em> front-end to the <em>ScaLapack</em> package; and</li>
<li>using MPI explicitly (in R, Python and C).</li>
</ul>
</div>
<div id="other-type-of-parallel-processing" class="slide section level1">
<h1>Other type of parallel processing</h1>
<p>We won't cover either of these in this material.</p>
<h2 id="gpus">GPUs</h2>
<p>GPUs (Graphics Processing Units) are processing units originally designed for rendering graphics on a computer quickly. This is done by having a large number of simple processing units for massively parallel calculation. The idea of general purpose GPU (GPGPU) computing is to exploit this capability for general computation.</p>
<p>In spring 2016, I gave a <a href="http://statistics.berkeley.edu/computing/gpu">workshop on using GPUs</a>.</p>
<h2 id="spark-and-hadoop">Spark and Hadoop</h2>
<p>Spark and Hadoop are systems for implementing computations in a distributed memory environment, using the MapReduce approach. In fall 2014, I gave a <a href="http://statistics.berkeley.edu/computing/gpu">workshop on using Spark</a>.</p>
</div>
<div id="parallelization-strategies" class="slide section level1">
<h1>Parallelization strategies</h1>
<p>The following are some basic principles/suggestions for how to parallelize your computation.</p>
</div>
<div id="parallelization-strategies-should-i-use-one-machinenode-or-many-machinesnodes" class="slide section level1">
<h1>Parallelization strategies: <em>Should I use one machine/node or many machines/nodes?</em></h1>
<ul>
<li>If you can do your computation on the cores of a single node using shared memory, that will be faster than using the same number of cores (or even somewhat more cores) across multiple nodes. Similarly, jobs with a lot of data/high memory requirements that one might think of as requiring Spark or Hadoop may in some cases be much faster if you can find a single machine with a lot of memory.</li>
<li>That said, if you would run out of memory on a single node, then you'll need to use multiple nodes.</li>
</ul>
</div>
<div id="parallelization-strategies-what-level-or-dimension-should-i-parallelize-over" class="slide section level1">
<h1>Parallelization strategies: <em>What level or dimension should I parallelize over?</em></h1>
<ul>
<li>If you have nested loops, you generally only want to parallelize at one level of the code. That said, there may be cases in which it is helpful to do both. Keep in mind whether your linear algebra is being threaded. Often you will want to parallelize over a loop and not use threaded linear algebra.</li>
<li>Often it makes sense to parallelize the outer loop when you have nested loops.</li>
<li>To parallelize over multiple levels, it's usually best to 'flatten' the indexing and not explicitly do nested parallelization.</li>
<li>You generally want to parallelize in such a way that your code is load-balanced and does not involve too much communication.</li>
</ul>
</div>
<div id="parallelization-strategies-how-do-i-balance-communication-overhead-with-keeping-my-cores-busy" class="slide section level1">
<h1>Parallelization strategies: <em>How do I balance communication overhead with keeping my cores busy?</em></h1>
<ul>
<li>If you have very few tasks, particularly if the tasks take different amounts of time, often some processors will be idle and your code poorly load-balanced.</li>
<li>If you have very many tasks and each one takes little time, the communication overhead of starting and stopping the tasks will reduce efficiency.</li>
</ul>
</div>
<div id="parallelization-strategies-should-multiple-tasks-be-pre-assigned-to-a-process-i.e.-a-worker-or-should-tasks-be-assigned-dynamically-as-previous-tasks-finish" class="slide section level1">
<h1>Parallelization strategies: <em>Should multiple tasks be pre-assigned to a process (i.e., a worker) or should tasks be assigned dynamically as previous tasks finish?</em></h1>
<ul>
<li>Basically if you have many tasks that each take similar time, you want to preschedule the tasks to reduce communication. If you have few tasks or tasks with highly variable completion times, you don't want to preschedule, to improve load-balancing.</li>
<li>For R in particular, some of R's parallel functions allow you to say whether the tasks should be prescheduled. E.g., the <em>mc.preschedule</em> argument in <em>mclapply</em> and the <em>.scheduling</em> argument in <em>parLapply</em>.</li>
<li>In iPython, dynamic assignment is called a <em>load-balanced view</em>, while prescheduling is called a <em>direct view</em>.</li>
</ul>
</div>
<div id="basic-parallelized-loopsmapsapply---overview" class="slide section level1">
<h1>Basic parallelized loops/maps/apply - Overview</h1>
<p>All of the functionality discussed here applies <em>only</em> if the iterations/loops of your calculations can be done completely separately and do not depend on one another. This scenario is called an <em>embarrassingly parallel</em> computation. So coding up the evolution of a time series or a Markov chain is not possible using these tools. However, bootstrapping, random forests, simulation studies, cross-validation and many other statistical methods can be handled in this way.</p>
<p>Most languages you'll encounter (in particular <em>functional</em> languages) will have the ability to operate a single function on multiple input values, thereby generating multiple tasks. This is often called a <em>map</em> operation, though in R one would call it an <em>apply</em>.</p>
<p>Usually there is a master process that manages the mapping, including dispatching individual tasks to the workers and collecting all the results.</p>
<p>The key challenges that arise are:</p>
<ul>
<li>starting up the worker processes and making sure the master process is communicating with them,</li>
<li>making sure that the workers have the necessary data and have loaded needed packages,</li>
<li>handling additional arguments to the function being used,</li>
<li>ensuring use of distinct random numbers in the different tasks, and</li>
<li>debugging code that operates in another process.</li>
</ul>
</div>
<div id="parallel-loops-and-apply-functions-on-one-machine-in-r" class="slide section level1">
<h1>Parallel loops and <em>apply</em> functions on one machine in R</h1>
<p>Demo code below is also in various R files in the repository.</p>
</div>
<div id="parallel-for-loops-with-foreach-on-one-machine" class="slide section level1">
<h1>Parallel for loops with <em>foreach</em> on one machine</h1>
<p>A simple way to exploit parallelism in R is to use the <em>foreach</em> package to do a for loop in parallel.</p>
<p>The <em>foreach</em> package provides a <em>foreach</em> command that allows you to do this easily. <em>foreach</em> can use a variety of parallel ``back-ends''. For our purposes, the main one is use of the <em>parallel</em> package to use shared memory cores. When using <em>parallel</em> as the back-end, you should see multiple processes (as many as you registered; ideally each at 100%) when you monitor CPU usage. The multiple processes are created by forking or using sockets.</p>
<pre><code>library(doParallel)  # uses parallel package, a core R package
# library(multicore); library(doMC) # alternative to parallel/doParallel
# library(Rmpi); library(doMPI) # to use Rmpi as the back-end

source(&#39;rf.R&#39;)  # loads in data and looFit()

looFit

nCores &lt;- 4  # to set manually
registerDoParallel(nCores) 
# registerDoMC(nCores) # alternative to registerDoParallel
# cl &lt;- startMPIcluster(nCores); registerDoMPI(cl) # when using Rmpi as the back-end

nSub &lt;- 30  # do only first 30 for illustration

result &lt;- foreach(i = 1:nSub) %dopar% {
    cat(&#39;Starting &#39;, i, &#39;th job.\n&#39;, sep = &#39;&#39;)
    output &lt;- looFit(i, Y, X)
    cat(&#39;Finishing &#39;, i, &#39;th job.\n&#39;, sep = &#39;&#39;)
    output # this will become part of the out object
}
print(result[1:5])</code></pre>
<p>(Note that the printed statements from <code>cat</code> are not showing up in the creation of this document but should show if you run the code.)</p>
<p>Note that <em>foreach</em> also provides functionality for collecting and managing the results to avoid some of the bookkeeping you would need to do if writing your own standard for loop. The result of <em>foreach</em> will generally be a list, unless we request the results be combined in different way, using the <code>.combine</code> argument.</p>
</div>
<div id="parallel-apply-functionality-in-r-on-one-machine" class="slide section level1">
<h1>Parallel apply functionality in R on one machine</h1>
<p>The <em>parallel</em> package has the ability to parallelize the various <em>apply</em> functions (apply, lapply, sapply, etc.). It's a bit hard to find the <a href="http://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf">vignette for the parallel package</a> because parallel is not listed as one of the contributed packages on CRAN (it gets installed with R by default).</p>
<p>We'll consider parallel lapply and sapply. These rely on having started a cluster using <em>cluster</em>, which uses the PSOCK mechanism as in the SNOW package - starting new jobs via <em>Rscript</em> and communicating via a technology called sockets.</p>
<pre><code>library(parallel)
nCores &lt;- 4  # to set manually 
cl &lt;- makeCluster(nCores) 

source(&#39;rf.R&#39;)  # loads in data and looFit()

nSub &lt;- 30
input &lt;- seq_len(nSub) # same as 1:nSub but more robust

# clusterExport(cl, c(&#39;x&#39;, &#39;y&#39;)) # if the processes need objects
# from master&#39;s workspace (not needed here as no global vars used)


# need to load randomForest package within function
# when using par{L,S}apply
system.time(
    res &lt;- parSapply(cl, input, looFit, Y, X, TRUE)
)
system.time(
    res2 &lt;- sapply(input, looFit, Y, X)
)

res &lt;- parLapply(cl, input, looFit, Y, X, TRUE)</code></pre>
<p>Here the miniscule user time is probably because the time spent in the worker processes is not counted at the level of the overall master process that dispatches the workers.</p>
<p>For help with these functions and additional related parallelization functions (including <em>parApply</em>), see <code>help(clusterApply)</code>.</p>
<p><em>mclapply</em> is an alternative that uses forking to start up the worker processes.</p>
<pre><code>source(&#39;rf.R&#39;)  # loads in data and looFit()

nSub &lt;- 30
input &lt;- seq_len(nSub) # same as 1:nSub but more robust

system.time(
    res &lt;- mclapply(input, looFit, Y, X, mc.cores = nCores) 
)</code></pre>
<p>Note that some R packages can directly interact with the parallelization packages to work with multiple cores. E.g., the <em>boot</em> package can make use of the <em>parallel</em> package directly.</p>
</div>
<div id="loading-packages-and-accessing-global-variables-within-your-parallel-tasks-in-r" class="slide section level1">
<h1>Loading packages and accessing global variables within your parallel tasks in R</h1>
<p>Whether you need to explicitly load packages and export global variables from the master process to the parallelized worker processes depends on the details of how you are doing the parallelization.</p>
<p>In R, with <em>foreach</em> with the <em>doParallel</em> backend, parallel <em>apply</em> statements (starting the cluster via <em>makeForkCluster</em>, instead of the usual <em>makeCluster</em>), and <em>mclapply</em>, packages and global variables in the main R process are automatically available to the worker tasks without any work on your part. This is because all of these approaches fork the original R process, thereby creating worker processes with the same state as the original R process. Interestingly, this means that global variables in the forked worker processes are just references to the objects in memory in the original R process. So the additional processes do not use additional memory for those objects (despite what is shown in <em>top</em>) and there is no time involved in making copies. However, if you modify objects in the worker processes then copies are made.</p>
<p>In contrast, with parallel <em>apply</em> statements when starting the cluster using the standard <em>makeCluster</em> (which sets up a so-called <em>PSOCK</em> cluster, starting the R worker processes via <em>Rscript</em>), one needs to load packages within the code that is executed in parallel. In addition one needs to use <em>clusterExport</em> to tell R which objects in the global environment should be available to the worker processes. This involves making as many copies of the objects as there are worker processes, so one can easily exceed the physical memory (RAM) on the machine if one has large objects, and the copying of large objects will take time.</p>
</div>
<div id="parallel-looping-in-python" class="slide section level1">
<h1>Parallel looping in Python</h1>
<p>Demo code below is also in <em>ipython-parallel.py</em>.</p>
<p>I'll cover iPython parallel functionality, which allows one to parallelize on a single machine (discussed here) or across multiple machines (see the tutorial on distributed memory parallelization). There are a variety of other approaches one could use, of which I discuss two (the <em>pp</em> and <em>multiprocessing</em> packages) in the distributed memory tutorial linked to early in this document.</p>
<p>First we need to start our worker engines.</p>
<pre><code>ipcluster start -n 4 &amp;
sleep 45</code></pre>
<p>Here we'll generate some fake data to fit a random forest model to and then use leave-one-out cross-validation to assess the model, parallelizing over the individual held-out observations.</p>
<pre><code>import numpy as np
np.random.seed(0)
n = 500
p = 50
X = np.random.normal(0, 1, size = (n, p))
Y = X[: , 0] + pow(abs(X[:,1] * X[:,2]), 0.5) + X[:,1] - X[:,2] + np.random.normal(0, 1, n)

def looFit(index, Ylocal, Xlocal):
    rf = rfr(n_estimators=100)
    fitted = rf.fit(np.delete(Xlocal, index, axis = 0), np.delete(Ylocal, index))
    pred = rf.predict(np.array([Xlocal[index, :]]))
    return(pred[0])

from ipyparallel import Client
c = Client()
c.ids

dview = c[:]
dview.block = True
dview.apply(lambda : &quot;Hello, World&quot;)

lview = c.load_balanced_view()
lview.block = True

dview.execute(&#39;from sklearn.ensemble import RandomForestRegressor as rfr&#39;)
dview.execute(&#39;import numpy as np&#39;)
mydict = dict(X = X, Y = Y, looFit = looFit)
dview.push(mydict)

nSub = 50  # for illustration only do a subset

# need a wrapper function because map() only operates on one argument
def wrapper(i):
    return(looFit(i, Y, X))

import time
time.time()
pred = lview.map(wrapper, range(nSub))
time.time()

print(pred[0:10])

# import pylab
# import matplotlib.pyplot as plt
# plt.plot(Y, pred, &#39;.&#39;)
# pylab.show()</code></pre>
<p>Finally we stop the worker engines:</p>
<pre><code>ipcluster stop</code></pre>
</div>
<div id="loading-packages-and-accessing-global-variables-within-your-parallel-tasks-in-ipython" class="slide section level1">
<h1>Loading packages and accessing global variables within your parallel tasks in iPython</h1>
<p>In iPython parallel, you need to do some work to ensure that data and packages are available on the workers.</p>
<ul>
<li>Package loading needs to be done by an explicit parallel call as seen above.</li>
<li>Since map() only operates on a single input, we write a wrapper function that operates only on the index value and that calls the real function that runs on the workers and uses data objects previously broadcast to the workers.</li>
</ul>
</div>
<div id="basic-parallelized-loopsmapsapply-across-multiple-machines" class="slide section level1">
<h1>Basic parallelized loops/maps/apply across multiple machines</h1>
<p>Here are some of the approaches that are possible on multiple machines:</p>
<ul>
<li>foreach in R with either doMPI or doSNOW backends</li>
<li>parallel apply in R with a multi-machine cluster of workers</li>
<li>iPython parallel with engines running on multiple nodes</li>
</ul>
</div>
<div id="foreach-with-dompi" class="slide section level1">
<h1>foreach with <em>doMPI</em></h1>
<p>Just as we used <em>foreach</em> in a shared memory context, we can use it in a distributed memory context as well, and R will handle everything behind the scenes for you.</p>
<p>Start R through the <em>mpirun</em> command as discussed above, either as a batch job or for interactive use. We'll only ask for 1 process because the worker processes will be started automatically from within R (but using the machine names information passed to mpirun).</p>
<p>When using this within the SLURM scheduling software (used on the newer SCF cluster and on Savio), we don't need to specify the machines to be used because SLURM and MPI are integrated.</p>
<pre><code>mpirun R CMD BATCH -q --no-save foreach-doMPI.R foreach-doMPI.out</code></pre>
<p>If one were doing this outside of SLURM, one may need to do this so that MPI knows the machines (and number of cores per machine) that are available:</p>
<pre><code>mpirun -machinefile .hosts R CMD BATCH -q --no-save foreach-doMPI.R \
       foreach-doMPI.out
mpirun -machinefile .hosts R --no-save</code></pre>
<p>where <em>.hosts</em> would look something like this:</p>
<pre><code>arwen.berkeley.edu slots=3
beren.berkeley.edu slots=2</code></pre>
<p>Here's the R code for using <em>Rmpi</em> as the back-end to <em>foreach</em>. If you call <em>startMPIcluster</em> with no arguments, it will start up one fewer worker processes than the number of hosts times slots given to mpirun so your R code will be more portable.</p>
<pre><code>library(Rmpi)
library(doMPI)

cl = startMPIcluster()  # by default will start one fewer slave
# than elements in .hosts
                                        
source(&#39;rf.R&#39;)  # loads in data and looFit()

registerDoMPI(cl)
clusterSize(cl) # just to check

nSub &lt;- 30  # do only first 30 for illustration

result &lt;- foreach(i = 1:nSub, .packages = &#39;randomForest&#39;) %dopar% {
    cat(&#39;Starting &#39;, i, &#39;th job.\n&#39;, sep = &#39;&#39;)
    output &lt;- looFit(i, Y, X)
    cat(&#39;Finishing &#39;, i, &#39;th job.\n&#39;, sep = &#39;&#39;)
    output # this will become part of the out object
}
print(result[1:5])

closeCluster(cl)

mpi.quit()</code></pre>
<p>A caution concerning Rmpi/doMPI: when you invoke <code>startMPIcluster()</code>, all the slave R processes become 100% active and stay active until the cluster is closed. In addition, when <em>foreach</em> is actually running, the master process also becomes 100% active. So using this functionality involves some inefficiency in CPU usage. This inefficiency is not seen with a doSNOW cluster setup with <code>type = &quot;SOCK&quot;</code> nor when using other Rmpi functionality - i.e., starting slaves with <em>mpi.spawn.Rslaves</em> and then issuing commands to the slaves.</p>
</div>
<div id="foreach-with-dosnow" class="slide section level1">
<h1>foreach with doSNOW</h1>
<p>The <em>doSNOW</em> backend has the advantage that it doesn't need to have MPI installed on the system. MPI can be tricky to install and keep working, so this is an easy approach to using <em>foreach</em> across multiple machines.</p>
<p>Simply start R as you usually would.</p>
<p>Here's R code for using <em>doSNOW</em> as the back-end to <em>foreach</em>. Make sure to use the <code>type = &quot;SOCK&quot;</code> argument or <em>doSNOW</em> will actually use MPI behind the scenes.</p>
<pre><code>library(doSNOW)
machines = c(rep(&quot;beren.berkeley.edu&quot;, 1),
    rep(&quot;gandalf.berkeley.edu&quot;, 1),
    rep(&quot;arwen.berkeley.edu&quot;, 2))

cl = makeCluster(machines, type = &quot;SOCK&quot;)
cl

registerDoSNOW(cl)

source(&#39;rf.R&#39;)  # loads in data and looFit()

nSub &lt;- 30  # do only first 30 for illustration

result &lt;- foreach(i = 1:nSub, .packages = &#39;randomForest&#39;) %dopar% {
    cat(&#39;Starting &#39;, i, &#39;th job.\n&#39;, sep = &#39;&#39;)
    output &lt;- looFit(i, Y, X)
    cat(&#39;Finishing &#39;, i, &#39;th job.\n&#39;, sep = &#39;&#39;)
    output # this will become part of the out object
}
print(result[1:5])

stopCluster(cl)  # good practice, but not strictly necessary</code></pre>
</div>
<div id="loading-packages-and-accessing-variables-within-your-parallel-r-tasks" class="slide section level1">
<h1>Loading packages and accessing variables within your parallel R tasks</h1>
<p>When using <em>foreach</em> with multiple machines, you need to use the <em>.packages</em> argument (or load the package in the code being run in parallel) to load any packages needed in the code.</p>
<p>You do not need to explicitly export variables from the master process to the workers. Rather, <em>foreach</em> determines which variables in the global environment of the master process are used in the code being run in parallel and makes copies of those in each worker process. Note that these variables are read-only on the workers and cannot be modified (if you try to do so, you'll notice that <em>foreach</em> actually did not make copies of the variables that your code tries to modify).</p>
</div>
<div id="using-parallel-apply-in-r-with-makecluster-on-multiple-nodes" class="slide section level1">
<h1>Using parallel apply in R with makeCluster on multiple nodes</h1>
<p>One can also set up a cluster with the worker processes communicating via sockets. You just need to specify a character vector with the machine names as the input to <em>makeCluster()</em>. A nice thing about this is that it doesn't involve any of the complications of working with needing MPI installed.</p>
<pre><code>library(parallel)

machines = c(rep(&quot;beren.berkeley.edu&quot;, 1),
    rep(&quot;gandalf.berkeley.edu&quot;, 1),
    rep(&quot;arwen.berkeley.edu&quot;, 2))
cl = makeCluster(machines)
cl

source(&#39;rf.R&#39;)  # loads in data and looFit()

nSub &lt;- 30
input &lt;- seq_len(nSub)

# not needed because Y and X are arguments,
# but would be needed if they were used as global variables:
# clusterExport(cl, c(&#39;Y&#39;, &#39;X&#39;))

  
result &lt;- parSapply(cl, input, looFit, Y, X, TRUE)

result[1:5]

stopCluster(cl) # not strictly necessary</code></pre>
<p>Note the need to load the packages used in the parallelized function on each of the nodes.</p>
</div>
<div id="distributed-parallel-looping-in-python" class="slide section level1">
<h1>Distributed parallel looping in Python</h1>
<p>One can use iPython's parallelization tools in a context with multiple nodes, though the setup to get the worker processes is a bit more involved when you have multiple nodes.</p>
<p>If we are using the SLURM scheduling software, here's how we start up the worker processes:</p>
<pre><code>ipcontroller --ip=&#39;*&#39; &amp;
sleep 25
# next line will start as many ipengines as we have SLURM tasks 
#   because srun is a SLURM command
srun ipengine &amp;  
sleep 45  # wait until all engines have successfully started</code></pre>
<p>We can then run iPython to split up our computational tasks across the engines, using the same code as we used in the single-node context above.</p>
<p>To finish up, we need to shut down the cluster of workers:</p>
<pre><code>ipcluster stop</code></pre>
<p>To start the engines in a context outside of using slurm (provided all machines share a filesystem), you should be able ssh to each machine and run <code>ipengine &amp;</code> for as many worker processes as you want to start as follows. In some, but not all cases (depending on how the network is set up) you may not need the <code>--location</code> flag.</p>
<pre><code>ipcontroller --ip=&#39;*&#39; --location=URL_OF_THIS_MACHINE &amp;
sleep 25
nengines=8
ssh other_host &quot;for (( i = 0; i &lt; ${nengines}; i++ )); do ipengine &amp; done&quot;
sleep 45  # wait until all engines have successfully started</code></pre>
</div>
<div id="random-number-generation-rng-in-parallel---overview" class="slide section level1">
<h1>Random number generation (RNG) in parallel - Overview</h1>
<p>The key thing when thinking about random numbers in a parallel context is that you want to avoid having the same 'random' numbers occur on multiple processes. On a computer, random numbers are not actually random but are generated as a sequence of pseudo-random numbers designed to mimic true random numbers. The sequence is finite (but very long) and eventually repeats itself. When one sets a seed, one is choosing a position in that sequence to start from. Subsequent random numbers are based on that subsequence. All random numbers can be generated from one or more random uniform numbers, so we can just think about a sequence of values between 0 and 1.</p>
<p>The worst thing that could happen is that one sets things up in such a way that every process is using the same sequence of random numbers. This could happen if you mistakenly set the same seed in each process, e.g., using <em>set.seed(mySeed)</em> in R on every process.</p>
<p>The naive approach is to use a different seed for each process. E.g., if your processes are numbered <code>id = 1,2,...,p</code> with a variable <em>id</em> that is unique to a process, setting the seed to be the value of <em>id</em> on each process. This is likely not to cause problems, but raises the danger that two (or more sequences) might overlap. For an algorithm with dependence on the full sequence, such as an MCMC, this probably won't cause big problems (though you likely wouldn't know if it did), but for something like simple simulation studies, some of your 'independent' samples could be exact replicates of a sample on another process. Given the period length of the default generators in R, Matlab and Python, this is actually quite unlikely, but it is a bit sloppy.</p>
<p>One approach to avoid the problem is to do all your RNG on one process and distribute the random deviates, but this can be infeasible with many random numbers.</p>
<p>More generally to avoid this problem, the key is to use an algorithm that ensures sequences that do not overlap.</p>
</div>
<div id="ensuring-separate-sequences-in-r" class="slide section level1">
<h1>Ensuring separate sequences in R</h1>
<p>In R, the <em>rlecuyer</em> package deals with this (<em>rsprng</em> used to but it is no longer on CRAN). The L'Ecuyer algorithm has a period of <img style="vertical-align:middle" src="http://chart.apis.google.com/chart?cht=tx&amp;chl=2%5E%7B191%7D" alt="2^{191}" title="2^{191}" />, which it divides into subsequences of length <img style="vertical-align:middle" src="http://chart.apis.google.com/chart?cht=tx&amp;chl=2%5E%7B127%7D" alt="2^{127}" title="2^{127}" />.</p>
<p>The code below is also in <em>parallelRNG.R</em>.</p>
<h2 id="with-the-parallel-package">With the parallel package</h2>
<p>Here's how you initialize independent sequences on different processes when using the <em>parallel</em> package's parallel apply functionality (illustrated here with <em>parSapply</em>).</p>
<pre><code>library(parallel)
library(rlecuyer)
nSims &lt;- 250
taskFun &lt;- function(i){
    val &lt;- runif(1)
    return(val)
}

nCores &lt;- 4
RNGkind()
cl &lt;- makeCluster(nCores)
iseed &lt;- 0
clusterSetRNGStream(cl = cl, iseed = iseed)
RNGkind() # clusterSetRNGStream sets RNGkind as L&#39;Ecuyer-CMRG
# but it doesn&#39;t show up here on the master
res &lt;- parSapply(cl, 1:nSims, taskFun)
# now redo with same master seed to see results are the same
clusterSetRNGStream(cl = cl, iseed = iseed)
res2 &lt;- parSapply(cl, 1:nSims, taskFun)
identical(res,res2)
stopCluster(cl)</code></pre>
<p>When using <em>mclapply</em>, you can use the <em>mc.set.seed</em> argument as follows (note that <em>mc.set.seed</em> is TRUE by default, so you should get different seeds for the different processes by default), but one needs to invoke <code>RNGkind(&quot;L'Ecuyer-CMRG&quot;)</code> to get independent streams via the L'Ecuyer algorithm.</p>
<pre><code>library(parallel)
library(rlecuyer)
RNGkind(&quot;L&#39;Ecuyer-CMRG&quot;)
res &lt;- mclapply(seq_len(nSims), taskFun, mc.cores = nCores, 
    mc.set.seed = TRUE) 
# this also seems to reset the seed when it is run
res2 &lt;- mclapply(seq_len(nSims), taskFun, mc.cores = nCores, 
    mc.set.seed = TRUE) 
identical(res,res2)</code></pre>
<p>The documentation for <em>mcparallel</em> gives more information about reproducibility based on <em>mc.set.seed</em>.</p>
</div>
<div id="ensuring-separate-sequences-in-r-with-foreach" class="slide section level1">
<h1>Ensuring separate sequences in R with foreach</h1>
<h2 id="getting-independent-streams">Getting independent streams</h2>
<p>One question is whether <em>foreach</em> deals with RNG correctly. This is not documented, but the developers (Revolution Analytics) are well aware of RNG issues. Digging into the underlying code reveals that the <em>doParallel</em> backend invokes <em>mclapply</em> and sets <em>mc.set.seed</em> to TRUE by default. This suggests that the discussion above r.e. <em>mclapply</em> holds for <em>foreach</em> as well, so you should do <code>RNGkind(&quot;L'Ecuyer-CMRG&quot;)</code> before your foreach call.</p>
<h2 id="ensuring-reproducibility">Ensuring reproducibility</h2>
<p>While using <em>foreach</em> as just described should ensure that the streams on each worker are are distinct, it does not ensure reproducibility because task chunks may be assigned to workers differently in different runs and the substreams are specific to workers, not to tasks.</p>
<p>For backends other than <em>doMPI</em>, such as <em>doParallel</em>, there is a package called <em>doRNG</em> that ensures that <em>foreach</em> loops are reproducible. (For <em>doMPI</em> you simply pass <code>.options.mpi = list(seed = your_seed_value_here)</code> as an additional argument to <em>foreach</em>.)</p>
<p>Here's how you do it:</p>
<pre><code>library(doRNG)
library(doParallel)
registerDoParallel(nCores)
registerDoRNG(seed = 0) 
result &lt;- foreach(i = 1:20) %dopar% { 
    out &lt;- mean(rnorm(1000)) 
}
registerDoRNG(seed = 0) 
result2 &lt;- foreach(i = 1:20) %dopar% { 
    out &lt;- mean(rnorm(1000)) 
}
identical(result,result2)</code></pre>
<p>You can ignore the warnings about closing unused connections printed out above.</p>
</div>
<div id="rng-in-python" class="slide section level1">
<h1>RNG in Python</h1>
<p>Python uses the Mersenne-Twister generator. If you're using the RNG in <em>numpy/scipy</em>, you can set the seed using <code>numpy.random.seed</code> or <code>scipy.random.seed</code>. The advice I'm seeing online in various Python forums is to just set separate seeds, so it appears the Python is a bit behind R and Matlab here. There is a function <em>random.jumpahead</em> that allows you to move the seed ahead as if a given number of random numbers had been generated, but this function will not be in Python 3.x, so I won't suggest using it.</p>
</div>
<div id="mpi-for-distributed-memory-computation" class="slide section level1">
<h1>MPI for distributed memory computation</h1>
<h2 id="mpi-overview">MPI Overview</h2>
<p>There are multiple MPI implementations, of which <em>openMPI</em> and <em>mpich</em> are very common. <em>openMPI</em> is on the SCF and Savio, and we'll use that.</p>
<p>In MPI programming, the same code runs on all the machines. This is called SPMD (single program, multiple data). One invokes the same code (same program) multiple times, but the behavior of the code can be different based on querying the rank (ID) of the process. Since MPI operates in a distributed fashion, any transfer of information between processes must be done explicitly via send and receive calls (e.g., <em>MPI_Send</em>, <em>MPI_Recv</em>, <em>MPI_Isend</em>, and <em>MPI_Irecv</em>). (The ``MPI_'' is for C code; C++ just has <em>Send</em>, <em>Recv</em>, etc.)</p>
<p>The latter two of these functions (<em>MPI_Isend</em> and <em>MPI_Irecv</em>) are so-called non-blocking calls. One important concept to understand is the difference between blocking and non-blocking calls. Blocking calls wait until the call finishes, while non-blocking calls return and allow the code to continue. Non-blocking calls can be more efficient, but can lead to problems with synchronization between processes.</p>
<p>In addition to send and receive calls to transfer to and from specific processes, there are calls that send out data to all processes (<em>MPI_Scatter</em>), gather data back (<em>MPI_Gather</em>) and perform reduction operations (<em>MPI_Reduce</em>).</p>
<p>Debugging MPI code can be tricky because communication can hang, error messages from the workers may not be seen or readily accessible, and it can be difficult to assess the state of the worker processes.</p>
</div>
<div id="basic-syntax-for-mpi-in-c" class="slide section level1">
<h1>Basic syntax for MPI in C</h1>
<p>Here's a basic hello world example The code is also in <em>mpiHello.c</em>.</p>
<pre><code>// see mpiHello.c
#include &lt;stdio.h&gt; 
#include &lt;math.h&gt; 
#include &lt;mpi.h&gt;

int main(int argc, char* argv) {     
    int myrank, nprocs, namelen;     
    char process_name[MPI_MAX_PROCESSOR_NAME];
    MPI_Init(&amp;argc, &amp;argv);     
    MPI_Comm_size(MPI_COMM_WORLD, &amp;nprocs);   
    MPI_Comm_rank(MPI_COMM_WORLD, &amp;myrank);          
    MPI_Get_processor_name(process_name, &amp;namelen);            
    printf(&quot;Hello from process %d of %d on %s\n&quot;, 
        myrank, nprocs, process_name);
    MPI_Finalize();     
    return 0; 
} </code></pre>
<p>There are C (<em>mpicc</em>) and C++ (<em>mpic++</em>) compilers for MPI programs (<em>mpicxx</em> and <em>mpiCC</em> are synonyms). I'll use the MPI C++ compiler even though the code is all plain C code.</p>
<pre><code>mpicxx mpiHello.c -o mpiHello
cat .hosts # what hosts do I expect it to run on?
mpirun -machinefile .hosts -np 4 mpiHello</code></pre>
<p>To actually write real MPI code, you'll need to go learn some of the MPI syntax. See <em>quad_mpi.c</em> and <em>quad_mpi.cpp</em>, which are example C and C++ programs (for approximating an integral via quadrature) that show some of the basic MPI functions. Compilation and running are as above:</p>
<pre><code>mpicxx quad_mpi.cpp -o quad_mpi
mpirun -machinefile .hosts -np 4 quad_mpi</code></pre>
</div>
<div id="using-mpi-from-python-and-r" class="slide section level1">
<h1>Using MPI from Python and R</h1>
<p>Both R (via Rmpi and pbdR) and Python (via mpi4py) allow you to make MPI calls within R and Python.</p>
<p>Here's some basic use of MPI within Python.</p>
<pre><code>from mpi4py import MPI
import numpy as np

comm = MPI.COMM_WORLD

# simple print out Rank &amp; Size
id = comm.Get_rank()
print &quot;Of &quot;, comm.Get_size() , &quot; workers, I am number &quot; , id, &quot;.&quot;

def f(id, n):
    np.random.seed(id)
    return np.mean(np.random.normal(0, 1, n))

n = 1000000
result = f(id, n)


output = comm.gather(result, root = 0)

if id == 0:
    print output</code></pre>
<p>To run the code, we start Python through the mpirun command as done previously.</p>
<pre><code>mpirun -machinefile .hosts -np 4 python example-mpi.py </code></pre>
<p>More generally, you can send, receive, broadcast, gather, etc. as with MPI itself.</p>
<p><em>mpi4py</em> generally does not work interactively.</p>
<p>The feel of using MPI via Rmpi is somewhat similar.</p>
</div>
</body>
</html>
